{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIB\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "DATA_DIR = os.sep.join(['bbc_news_Data','bbc-fulltext (document classification)','bbc'])\n",
    "CATEGORY = 'category'\n",
    "DOCUMENT_ID = 'document_id'\n",
    "#TEXT = 'text'\n",
    "TITLE = 'title' # remove this element for the demo\n",
    "#STORY = 'story'\n",
    "INPUT = 'input'\n",
    "LABEL = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>document_id</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label document_id                                              input\n",
       "0  business         001  Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1  business         002  Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2  business         003  Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3  business         004  High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4  business         005  Pernod takeover talk lifts Domecq\\n\\nShares in..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ Data from text\n",
    "frame = defaultdict(list)\n",
    "for dir_name, _, file_names in os.walk(DATA_DIR):\n",
    "    try:\n",
    "        file_names.remove('README.TXT')\n",
    "        file_names.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    for file_name in file_names:\n",
    "        frame[LABEL].append(os.path.basename(dir_name))\n",
    "        name = os.path.splitext(file_name)[0]\n",
    "        frame[DOCUMENT_ID].append(name)\n",
    "        path = os.path.join(dir_name, file_name)\n",
    "        with open(path,'r', encoding='unicode_escape') as file:\n",
    "            frame[INPUT].append(file.read())\n",
    "df = pd.DataFrame.from_dict(frame)\n",
    "\n",
    "# DATA STATUS NOW\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERTFY_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\23742\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2225/2225 [00:01<00:00, 1584.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, ..., 2, 0, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA Encoding\n",
    "\n",
    "# LABEL\n",
    "df[LABEL].unique()\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[LABEL].values)\n",
    "y\n",
    "\n",
    "\n",
    "# TEXT\n",
    "nltk.download('stopwords')\n",
    "stopword_e = stopwords.words('english')\n",
    "def clean_text(text):\n",
    "    # decontraction: https://stackoverflow.com/a/47091490/7445772\n",
    "    # specific\n",
    "    text =re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", 'can not', text)\n",
    "\n",
    "    #general\n",
    "    text = re.sub(r\"n\\'t\", 'not', text)\n",
    "    text = re.sub(r\"\\'re\", \"are\", text)\n",
    "    text = re.sub(r\"\\'s\", \"is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    # remove line breaks  \\r \\n \\t remove from string\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\\"', ' ')\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "\n",
    "    # remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word not in stopword_e)\n",
    "\n",
    "    # remove special words\n",
    "    text = re.sub('[^A-Za-z0-9]+', \" \", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "input_processed = []\n",
    "for txt in tqdm(df[INPUT].values):\n",
    "    txt_p = clean_text(txt)\n",
    "    input_processed.append(txt_p)\n",
    "\n",
    "#Vect.\n",
    "text_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "X = text_transformer.fit_transform(input_processed)\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y, test_size=0.2)\n",
    "X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy:  0.9730337078651685\n"
     ]
    }
   ],
   "source": [
    "# Model trtain\n",
    "model = linear_model.LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "joblib.dump(model,'LR_model')\n",
    "#model = joblib.load('LR_model')\n",
    "lr_prediction = model.predict(X_test)\n",
    "print('accurancy: ',accuracy_score(lr_prediction,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 233 features per sample; expecting 746340",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-fa4f5c61b171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LR_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37_t1x\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37_t1x\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 287\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 233 features per sample; expecting 746340"
     ]
    }
   ],
   "source": [
    "#Verify txt test\n",
    "vframe = defaultdict(list)\n",
    "with open('varify.txt','r', encoding='unicode_escape')as file:\n",
    "    vframe['txt'].append(file.read())\n",
    "vdf = pd.DataFrame.from_dict(vframe)\n",
    "vdf['txt'] = vdf['txt'].apply(lambda x: clean_text(x))\n",
    "x_v = text_transformer.fit_transform(vdf['txt'])\n",
    "model_l = joblib.load('LR_model')\n",
    "res = model_l.predict(x_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
